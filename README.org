# Created 2017-06-30 Fri 09:41
#+TITLE: pyBLP
#+DATE: [2017-06-30 Fri]
#+AUTHOR: Joon Ro
#+EMAIL: joon.ro@outlook.com
* Introduction
=BLP-Python= provides a Python implementation of random coefficient logit model
of Berry, Levinsohn and Pakes (1995).

Based on code for Nevo (2000b) at
http://faculty.wcas.northwestern.edu/~ane686/supplements/rc_dc_code.htm.

This code calculates analytical gradients and use tight tolerances for the
contraction mapping (Dube et al. 2012). With BFGS method, it quickly converges
to the optimum (see Nevo (2000b) Example).

I would like to thank Prof. Nevo and others for making their code available.
Also, I would like to thank [[http://wingware.com][Wingware]], who generously provided a free license
of [[http://wingware.com][WingIDE]] for this non-commercial open source project.
** Notes on the code
- Use global states only for read-only variables
- Avoid inverting matrices whenever possible for numerical stability
- Use tighter tolerance for the contraction mapping
- Use greek unicode symbols whenever possible for readability
- Fix a small bug in the original code that prints wrong standard error twice
  for the mean estimates
- Market share integration is done in C via Cython, and it is parallelized
  across the simulation draws via OpenMP
- Use n-dimensional arrays (via [[http://xarray.pydata.org][xarray]]) to represent data more naturally
* Installation
** Dependencies
- Python 3.5+ (for ~@~ operator and unicode variable names). I recommend
  [[https://www.continuum.io/downloads][Anaconda Python Distribution]], which comes with many of the scientific libraries,
  as well as =conda=, a convenient script to install many packages.
- =numpy= and =scipy= for array operations and linear algebra
- =cython= for parallelized market share integration
- =xarray= for multidimensional labeled arrays (does not come with Anaconda,
  install with =conda install xarray=)
- =pandas= for result printing
** Download
- With git:

  #+BEGIN_SRC sh
    git clone https://github.com/joonro/BLP-Python.git
  #+END_SRC

- Or you can download the [[https://github.com/joonro/BLP-Python/archive/master.zip][master branch]] as a zip archive
** Compiling the Cython Module
- I include the compiled Cython module (=_BLP.cp35-win_amd64.pyd=) for Python
  3.5 64bit, so you should be able to run the code without compiling the
  module in Windows. You have to compile it if you want to change the Cython
  module (=_BLP.pyx=) or if you are on GNU/Linux or Mac OS. GNU/Linux
  distributions come with =gcc= so it should be straightforward to compile the
  module.
- ~cd~ into the =BLP-Python= directory, and compile the cython module with
  the following command:

  #+BEGIN_SRC sh
    python setup.py build_ext --inplace
  #+END_SRC
*** Windows
- For Windows users, to compile the cython module with the OpenMP
  (parallelization) support with 64-bit Python, you have to install Microsoft
  Visual C++ compiler following instructions at
  https://wiki.python.org/moin/WindowsCompilers. For Python 3.5, you either
  install Microsoft Visual C++ 14.0 standalone, or you can install Visual
  Studio 2015 which contains Visual C++ 14.0 compiler.
* Nevo (2000b) Example
=examples/Nevo_2000b.py= replicates the results from Nevo
(2000b). In the =examples= folder, you can run the script as:

#+BEGIN_SRC sh
python ./Nevo_2000b.py
#+END_SRC

It evaluates the objective function at the starting values and creates the
following results table:

#+BEGIN_SRC python
               Mean        SD      Income  Income^2       Age     Child
Constant  -1.833294  0.377200    3.088800  0.000000  1.185900   0.00000
           0.257829  0.129433    1.212647  0.000000  1.012354   0.00000
Price    -32.446922  1.848000   16.598000 -0.659000  0.000000  11.62450
           7.751913  1.078371  172.776110  8.979257  0.000000   5.20593
Sugar      0.142915 -0.003500   -0.192500  0.000000  0.029600   0.00000
           0.012877  0.012297    0.045528  0.000000  0.036563   0.00000
Mushy      0.801608  0.081000    1.468400  0.000000 -1.514300   0.00000
           0.203454  0.206025    0.697863  0.000000  1.098321   0.00000
GMM objective: 14.900789417017275
Min-Dist R-squared: 0.2718388379589566
Min-Dist weighted R-squared: 0.0946528053333926
#+END_SRC

Note that standard errors are slightly different because I avoid inverting
matrices as much as possible in calculations. In addition, the original code
has a minor bug in the standard error printing. That is, in =rc_dc.m=, line
102, ~semcoef = [semd(1); se(1); semd];~ should be ~semcoef = [semd(1); se(1);
semd(2:3)];~ instead (=0.258= is printed twice as a result).

This code uses tighter tolerance for the contraction mapping, and this code
minimizes the GMM objective function to the correct minimum of =4.56=. (With
BFGS, it only needs 45 iterations).

After running the code, you can try the full estimation with:

#+BEGIN_SRC python
BLP.estimate(θ20=θ20)
#+END_SRC

For example, in an IPython console:

#+BEGIN_SRC python
%run Nevo_2000b.py
BLP.estimate(θ20=θ20)
#+END_SRC

You should get the following results:

#+BEGIN_SRC python
Optimization terminated successfully.
         Current function value: 4.561515
         Iterations: 45
         Function evaluations: 50
         Gradient evaluations: 50

               Mean        SD      Income   Income^2       Age      Child
Constant  -2.009919  0.558094    2.291972   0.000000  1.284432   0.000000
           0.326997  0.162533    1.208569   0.000000  0.631215   0.000000
Price    -62.729902  3.312489  588.325237 -30.192021  0.000000  11.054627
          14.803215  1.340183  270.441021  14.101230  0.000000   4.122563
Sugar      0.116257 -0.005784   -0.384954   0.000000  0.052234   0.000000
           0.016036  0.013505    0.121458   0.000000  0.025985   0.000000
Mushy      0.499373  0.093414    0.748372   0.000000 -1.353393   0.000000
           0.198582  0.185433    0.802108   0.000000  0.667108   0.000000
GMM objective: 4.5615146550344186
Min-Dist R-squared: 0.4591043336106454
Min-Dist weighted R-squared: 0.10116438381046189
#+END_SRC

You can check the gradient at the optimum:

#+BEGIN_SRC python
>>> BLP._gradient_GMM(BLP.results['θ2']['x'])
contraction mapping finished in 0 iterations

array([  1.23888940e-07,   1.15056001e-08,   1.58824491e-08,
        -4.45649242e-08,  -9.61452074e-08,  -1.75233503e-08,
        -9.94539619e-07,   9.60900497e-08,  -3.30553299e-07,
         1.24174991e-07,   4.17569410e-07,   1.33642515e-07,
         1.94273594e-09])
#+END_SRC

I verified that the optimum is achieved with =Nelder-Mead= (simplex), =BFGS=,
=TNC=, and =SLSQP= [[https://www.docs.scipy.org/doc/scipy/reference/optimize.html][=scipy.optimize=]] methods. =BFGS= and =SLSQP= were the
fastest, and =BFGS= is the default.

* Unit Testing
I use =pytest= for unit testing. You can run them with:

#+BEGIN_SRC python
python -m pytest
#+END_SRC

* References
Berry, S., Levinsohn, J., & Pakes, A. (1995). /Automobile Prices In Market
Equilibrium/. Econometrica, 63(4), 841.

Dubé, J., Fox, J. T., & Su, C. (2012). Improving the Numerical Performance of
BLP Static and Dynamic Discrete Choice Random Coefficients Demand
Estimation. Econometrica, 1–34.

Nevo, A. (2000). /A Practitioner’s Guide to Estimation of Random-Coefficients
Logit Models of Demand/. Journal of Economics & Management Strategy, 9(4),
513–548.
* License
BLP-Python is released under the GPLv3.
* Changelog
** 0.4.2 ([2017-06-30 Fri])
- Fix =setup.py= for the Cython module for non-windows operating systems (thanks to [[https://github.com/cniedotus][Cheng Nie]])
** 0.4.0 ([2016-12-18 Sun])
- Use global state only for read-only variables; now gradient-based
  optimization (such as BFGS) works and it converges quickly
- Use pandas.DataFrame to show results cleanly
- Implement estimation of parameter means
- Implement standard error calculation
- Use greek letters whenever possible
- Add Nevo (2000b) example
- Add a unit test
- Improve README
** 0.3.0 ([2014-11-28 Fri])
- Implement GMM objective function and estimation of \( \theta_{2} \)
** 0.1.0 ([2013-03-28 Thu])
- Initial release
